@article{Rush2015,
abstract = {Summarization based on text extraction is inherently limited, but generation-style ab-stractive methods have proven challeng-ing to build. In this work, we propose a fully data-driven approach to abstrac-tive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary con-ditioned on the input sentence. While the model is structurally simple, it can eas-ily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines.},
author = {Rush, Alexander M and Chopra, Sumit and Weston, Jason},
file = {:home/spider-dawg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Rush, Chopra, Weston - 2015 - A Neural Attention Model for Sentence Summarization.pdf:pdf},
pages = {379--389},
title = {{A Neural Attention Model for Sentence Summarization}},
url = {http://www.aclweb.org/anthology/D15-1044},
year = {2015}
}
@article{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to var-ious natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational re-quirements.},
author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'{e}}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
file = {:home/spider-dawg/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Collobert et al. - 2011 - Natural Language Processing (Almost) from Scratch.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {natural language processing,neural networks},
pages = {2493--2537},
title = {{Natural Language Processing (Almost) from Scratch}},
url = {http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf},
volume = {12},
year = {2011}
}
@misc{Tavernise2016,
abstract = {The proliferation of fake and hyperpartisan news that has flooded into Americans' laptops and living rooms has prompted a national soul-searching, with liberals across the country asking how a nation of millions could be marching to such a suspect drumbeat. But while some Americans may take the stories literally — like the North Carolina man who fired his gun in a Washington pizzeria on Sunday trying to investigate a false story spread online of a child-abuse ring led by Hillary Clinton — many do not.},
author = {Tavernise, Sabrina},
booktitle = {New York Times},
title = {{As Fake News Spreads Lies, More Readers Shrug at the Truth - The New York Times}},
url = {https://www.nytimes.com/2016/12/06/us/fake-news-partisan-republican-democrat.html},
urldate = {2017-11-25},
year = {2016}
}
@incollection{Joachims1998,
author = {Joachims, Thorsten},
doi = {10.1007/BFb0026683},
pages = {137--142},
title = {{Text categorization with Support Vector Machines: Learning with many relevant features}},
url = {http://link.springer.com/10.1007/BFb0026683},
year = {1998}
}
@book{Liu2017,
address = {Cham},
doi = {10.1007/978-3-319-70096-0},
editor = {Liu, Derong and Xie, Shengli and Li, Yuanqing and Zhao, Dongbin and El-Alfy, El-Sayed M.},
isbn = {978-3-319-70095-3},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Neural Information Processing}},
url = {http://link.springer.com/10.1007/978-3-319-70096-0},
volume = {10635},
year = {2017}
}
@incollection{Singhania2017,
author = {Singhania, Sneha and Fernandez, Nigel and Rao, Shrisha},
doi = {10.1007/978-3-319-70096-0_59},
pages = {572--581},
title = {{3HAN: A Deep Neural Network for Fake News Detection}},
url = {http://link.springer.com/10.1007/978-3-319-70096-0{\_}59},
year = {2017}
}
@misc{Brownlee2016,
abstract = {There are two kinds of Americans: those who think Donald Trump says it like it is, and those who think that Donald Trump says it like a sputtering, sub-human gonad. No matter which camp you fall into, though, you might very well mistake the DeepDrumpf Twitter account for the real thing. Programmed by MIT's Bradley Hayes, DeepDrumpf leverages the power of neural networks to randomly generate tweets based on Trump's own bombastic, bloviating, bigoted speaking style!},
author = {Brownlee, John},
booktitle = {Co.design},
title = {{MIT's DeepDrumpf Twitter Bot Uses Neural Networks To Tweet Like Donald}},
url = {https://www.fastcodesign.com/3057501/mits-deepdrumpf-twitter-bot-uses-neural-networks-to-tweet-like-donald-trump},
urldate = {2017-11-25},
year = {2016}
}
@misc{Knight2017,
abstract = {Detecting the sentiment of social-media posts is already useful for tracking attitudes toward brands and products, and for identifying signals that might indicate trends in the financial markets. But more accurately discerning the meaning of tweets and comments could help computers automatically spot and quash abuse and hate speech online. A deeper understanding of Twitter should also help academics understand how information and influence flows through the network. What's more, as machines become smarter, the ability to sense emotion could become an important feature of human-to-machine communication.},
author = {Knight, Will},
booktitle = {MIT Technology Review},
title = {{An Algorithm Trained on Emoji Knows When You're Being Sarcastic on Twitter - MIT Technology Review}},
url = {https://www.technologyreview.com/s/608387/an-algorithm-trained-on-emoji-knows-when-youre-being-sarcastic-on-twitter/},
urldate = {2017-11-25},
year = {2017}
}
@misc{Davydova,
abstract = {NLP includes a wide set of syntax, semantics, discourse, and speech tasks. We will describe prime tasks in which neural networks demonstrated state-of-the-art performance.},
author = {Davydova, Olga},
booktitle = {Medium},
title = {{10 Applications of Artificial Neural Networks in Natural Language Processing}},
url = {https://medium.com/@datamonsters/artificial-neural-networks-in-natural-language-processing-bcf62aa9151a},
urldate = {2017-11-25}
}
@misc{Davydovaa,
abstract = {What is an artificial neural network? How does it work? What types of artificial neural networks exist? How are different types of artificial neural networks used in natural language processing? We will discuss all these questions in the following article.},
author = {Davydova, Olga},
booktitle = {Medium},
title = {{7 types of Artificial Neural Networks for Natural Language Processing}},
url = {https://medium.com/@datamonsters/artificial-neural-networks-for-natural-language-processing-part-1-64ca9ebfa3b2},
urldate = {2017-11-25}
}
@misc{Weller2017,
abstract = {Instead of absorbing meaning from their studies, Arai has observed children behaving more like her Todai robot. They ingest facts and spit them back out, without comprehension. The problem is, Todai and other forms of AI will inevitably surpass human memory and cognition at some point, research has suggested. The human brain can never compete with the rote fact-checking power of a computer.},
author = {Weller, Chris},
booktitle = {Business Insider},
title = {{Robot beat 80{\%} of students on University of Tokyo entrance exam - Business Insider}},
url = {http://www.businessinsider.com/robot-beat-most-students-on-university-tokyo-entrance-exam-2017-9},
urldate = {2017-11-25},
year = {2017}
}
